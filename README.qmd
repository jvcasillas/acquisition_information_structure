---
format: gfm
bibliography: information_structure.bib
execute: 
  echo: false
  message: false
  warning: false
  fig-dpi: 300
  fig-asp: 0.56
  out-width: "100%"
---

```{r}
#| label: setup
#| include: false
library("here")
library("rscopus")
library("tidyr")
library("dplyr")
library("readr")
library("glue")
library("stringr")
library("ggplot2")
library("forcats")
library("tidytext")
library("worldlanguages")
library("patchwork")
```


Scopus search using `rscopus` on the following terms: 

- Information Structure
- Information Packaging
- Topic
- Focus
- Information Focus
- Contrastive Focus
- New Information
- Old Information
- Given
- Givenness
- Discourse anaphor
- Contrastiveness
- Presupposition
- Theme
- Rheme
- Presentational Focus

```{r}
#| label: scopus-query
#| eval: false
#| echo: true
#
# Getting publication data via scopus
#
# set year thresholds
min_year <- 1960
max_year <- 2025

# prep empty output    
out_df <- NULL

# search by year (broken down by year because of 5k limit on records)
for(i in min_year:max_year){
  
  res <- scopus_search(
    query = paste(
      "TITLE-ABS-KEY({information structure} OR {information packaging} OR 
        topic OR focus OR {information focus} OR {contrastive focus} OR
        {new information} OR {old information} OR given OR givenness OR 
        {discourse anaphor} OR contrastiveness OR presupposition OR 
        theme OR rheme OR {presentational focus}) AND 
       PUBYEAR = ", i, "AND 
       SRCTYPE(j)"
    ),
    count = 25,
    max_count = 5000, 
    view = "COMPLETE"
  )

  n_records <- res$total_results
  
  if (n_records > 5000) {
    stop("Joseph, add a while() loop")
  }

  out_df <- bind_rows(out_df, gen_entries_to_df(res$entries)$df)
  
}

out_df |> 
  write_csv(here("data", "raw", glue("{Sys.Date()}_scopus_out.csv")))

```


```{r}
#| label: tidy-data
#| eval: false

src_is <- read_csv(here("data", "raw", "2025-03-13_scopus_out.csv")) |> 
  select(
    title        =  "dc:title",
    author       = "dc:creator",
    kw           = "authkeywords", 
    abstract     = "dc:description", 
    journal      = "prism:publicationName",
    issn         = "prism:issn",
    volume       = "prism:volume",
    pages        = "prism:pageRange",
    date         = "prism:coverDate",
    display_date = "prism:coverDisplayDate",
    doi          = "prism:doi",
    citations    = "citedby-count",
  ) |>
  mutate(
    year = lubridate::year(date), 
    journal = str_replace(journal, "&amp;", "&"), 
    kw = str_to_lower(kw), 
  ) |> 
  filter(!is.na(journal)) |> 
  write_csv(here("data", "tidy", "scopus_out_tidy.csv"))
```

In order to remove irrelevant hits, post-processsing also filtered the data set to only include the following journals: 

- Discourse Processes
- Journal of Pragmatics
- Psychological Science
- Journal of Phonetics
- Journal of Experimental Linguistics
- Applied Psycholinguistics
- Second Language Research
- Studies in Second Language Acquisition
- Bilingualism: Language and Cognition
- The Quarterly Journal of Experimental Psychology
- Cognition
- Journal of Memory and Language
- Journal of Experimental Psychology
- Learning, Memory, and Cognition
- Language, Cognition, and Neuroscience
- Behavioral and Brain Science
- Glossa
- Glossa Psycholinguistics
- Linguistic Approaches to Bilingualism
- Journal of Linguistics
- Natural Language and Linguistic Theory
- Linguistic Inquiry
- Probus
- Language, Cognition and Neuroscience
- Annual Review of Linguistics
- Frontiers in Language Science
- Frontiers in Psychology
- Languages
- Language
- Borealis
- Studia Linguistica
- Acta Linguistica Hungarica
- Linguistics

```{r}
#| label: play

journal_list <- list(
  j1 = list(
    journal = "Discourse Processes", 
    label = "Discourse Processes", 
    type = "experimental"
  ), 
  j2 = list(
    journal = "Journal of Pragmatics", 
    label = "Journal of Pragmatics", 
    type = "experimental"
  ), 
  j3 = list(
    journal = "Psychological Science", 
    label = "Psychological Science", 
    type = "experimental"
  ), 
  j4 = list(
    journal = "Journal of Phonetics", 
    label = "Journal of Phonetics",
    type = "experimental"
  ), 
  j5 = list(
    journal = "Journal of Experimental Linguistics", 
    label = "Journal of Experimental\nLinguistics", 
    type = "experimental"
  ), 
  j6 = list(
    journal = "Applied Psycholinguistics", 
    label = "Applied\nPsycholinguistics",
    type = "experimental"
  ), 
  j7 = list(
    journal = "Second Language Research", 
    label = "Second Language\nResearch",
    type = "experimental"
  ), 
  j8 = list(
    journal = "Studies in Second Language Acquisition", 
    label = "Studies in Second\nLanguage Acquisition",
    type = "experimental"
  ), 
  j9 = list(
    journal = "Bilingualism: Language and Cognition", 
    label = "Bilingualism: Language and Cognition",
    type = "experimental"
  ), 
  j10 = list(
    journal = "The Quarterly Journal of Experimental Psychology", 
    label = "The Quarterly Journal\nof Experimental Psychology",
    type = "experimental"
  ), 
  j11 = list(
    journal = "Cognition", 
    label = "Cognition",
    type = "experimental"
  ), 
  j12 = list(
    journal = "Journal of Memory and Language", 
    label = "Journal of Memory\nand Language",
    type = "experimental"
  ), 
  j13 = list(
    journal = "Journal of Experimental Psychology", 
    label = "Journal of Experimental\nPsychology",
    type = "experimental"
  ), 
  j14 = list(
    journal = "Learning, Memory, and Cognition", 
    label = "Learning, Memory,\nand Cognition",
    type = "experimental"
  ), 
  j15 = list(
    journal = "Language, Cognition, and Neuroscience", 
    label = "Language, Cognition,\nand Neuroscience",
    type = "experimental"
  ), 
  j16 = list(
    journal = "Behavioral and Brain Science", 
    label = "Behavioral and Brain Science",
    type = "experimental"
  ), 
  j17 = list(
    journal = "Glossa", 
    label = "Glossa",
    type = "experimental"
  ), 
  j18 = list(
    journal = "Glossa Psycholinguistics", 
    label = "Glossa Psycholinguistics",
    type = "experimental"
  ), 
  j19 = list(
    journal = "Linguistic Approaches to Bilingualism", 
    label = "Linguistic Approaches\nto Bilingualism",
    type = "experimental"
  ), 
  j20 = list(
    journal = "Journal of Linguistics", 
    label = "Journal of Linguistics",
    type = "theoretical"
  ), 
  j21 = list(
    journal = "Natural Language and Linguistic Theory", 
    label = "Natural Language and\nLinguistic Theory",
    type = "theoretical"
  ), 
  j22 = list(
    journal = "Linguistic Inquiry", 
    label = "Linguistic Inquiry",
    type = "theoretical"
  ), 
  j23 = list(
    journal = "Probus", 
    label = "Probus",
    type = "theoretical"
  ), 
  j24 = list(
    journal = "Language, Cognition and Neuroscience", 
    label = "Language, Cognition\nand Neuroscience",
    type = "experimental"
  ), 
  j25 = list(
    journal = "Annual Review of Linguistics", 
    label = "Annual Review\nof Linguistics",
    type = "experimental"
  ), 
  j26 = list(
    journal = "Frontiers in Language Science", 
    label = "Frontiers in Language Science",
    type = "experimental"
  ), 
  j27 = list(
    journal = "Frontiers in Psychology", 
    label = "Frontiers in Psychology",
    type = "experimental"
  ), 
  j28 = list(
    journal = "Languages", 
    label = "Languages",
    type = "experimental"
  ), 
  j29 = list(
    journal = "Language", 
    label = "Language",
    type = "theoretical"
  ), 
  j30 = list(
    journal = "Borealis", 
    label = "Borealis",
    type = "theoretical"
  ), 
  j31 = list(
    journal = "Studia Linguistica", 
    label = "Studia Linguistica",
    type = "theoretical"
  ), 
  j32 = list(
    journal = "Acta Linguistica Hungarica", 
    label = "Acta Linguistica Hungarica",
    type = "theoretical"
  ), 
  j33 = list(
    journal = "Linguistics", 
    label = "Linguistics",
    type = "theoretical"
  )
)

journal_filter <- bind_rows(journal_list) |> 
  mutate(journal = str_to_lower(journal)) |> 
  write_csv(here("data", "tidy", "journal_list.csv"))

remove_j <- c("frontiers in psychology", "journal of experimental psychology")

terms <- c(
  "information structure", "information packaging", "topic", "focus", 
  "information focus", "contrastive focus", "new information", 
  "old information", "given", "givenness", "discourse anaphor", 
  "contrastiveness", "presupposition", "theme", "rheme", 
  "presentational focus"
)

dat <- read_csv(here("data", "tidy", "scopus_out_tidy.csv")) |> 
  mutate(
    journal = str_to_lower(journal), 
    contains_kw = str_detect(title, paste(terms, collapse = "|"))
  ) |> 
  filter(
    #journal %in% journal_filter$journal, 
    #!(journal %in% remove_j), 
    contains_kw == TRUE
  ) |> 
  left_join(journal_filter, by = "journal")

n_hits <- nrow(dat)

hits_df <- function(x = 1960, y = 500, lab) {
  out <- tibble(
    x = x, 
    y = y, 
    lab = glue("Approx. {prettyNum(n_hits, big.mark = ',')} entries\n as of 03/01/2025")
  )
  
  return(out)
}

quant_df <- function(x = 1990, y = 500, lab) {
  out <- tibble(
    x = x, 
    y = y, 
    lab = glue("50% of entries\nafter {half_quant}")
  )
  
  return(out)
}

```


Next, we plot publications over time (@fig-line-graph): 


```{r}
#| label: fig-line-graph
#| fig-asp: 0.37
#| fig-dpi: 600
#| fig-cap: Number of articles per year (left) and cumulative totals in five-year intervals featuring information-structure terms in Scopus, 1960-2024. 

n_dat <- dat |> 
  group_by(year) |> 
  count() |> 
  ungroup() |> 
  complete(year = 1960:2025, fill = list(n = 0)) |> 
  mutate(cum_n = cumsum(n)) 

half_quant <- n_dat |> 
  select(year, n) |> 
  uncount(n) |> 
  mutate(count = 1) |> 
  pull(year) |> 
  quantile(0.5) 

pn_1 <- n_dat |> 
  filter(year < 2025) |> 
  ggplot() + 
  aes(x = year, y = n) + 
  geom_path(color = "#cc0033", linewidth = 0.7) +
  geom_point(size = 1.5, pch = 21, fill = "#cc0033", color = "white") + 
  geom_text(
    data = hits_df(x = 1960, y = 120), 
    aes(x = x, y = y, label = lab), 
    hjust = 0, size = 2
  ) + 
  #scale_x_continuous(breaks = seq(1960, 2025, 5)) + 
  scale_y_continuous(breaks = seq(0, 135, 15)) + 
  coord_cartesian(ylim = c(0, 135)) + 
  labs(x = NULL, y = "N") + 
  ds4ling::ds4ling_bw_theme(base_size = 10) + 
  theme(axis.title.y.left = element_text(hjust = 1))

pn_2 <- n_dat |> 
  filter(year %in% seq(1960, 2025, 5)) |> 
  ggplot() + 
  aes(x = year, y = cum_n) + 
  geom_vline(xintercept = half_quant, lty = 3, color = "#666666", linewidth = 0.5) + 
  geom_path(color = "#cc0033", linewidth = 0.7) +
  geom_point(size = 2, pch = 21, fill = "#cc0033", color = "white") + 
  geom_text(
    data = quant_df(x = 2000, y = 1000), 
    aes(x = x, y = y, label = lab), 
    hjust = 0, size = 2
  ) +
  #scale_x_continuous(breaks = seq(1960, 2025, 5)) + 
  scale_y_continuous(
    name = "N (cumulative)", position = "right", 
    breaks = seq(0, 2000, 250)
  ) + 
  coord_cartesian(ylim = c(0, 2000)) + 
  labs(x = NULL, y = "N") + 
  ds4ling::ds4ling_bw_theme(base_size = 10) + 
  theme(axis.title.y.right = element_text(hjust = 0))

pn_1 +  pn_2
```


We can also visualize the number of entries in a histogram (@fig-bar-graph, not included in the editorial): 

```{r}
#| label: fig-bar-graph
#| fig-cap: Number of articles per year in five-year intervals featuring information-structure terms in Scopus, 1960-2024. 

dat |> 
  ggplot() + 
  aes(x = year) + 
  geom_histogram(color = "white", fill = "#cc0033", binwidth = 5) + 
  geom_text(
    data = hits_df(x = 1965, y = 350), 
    aes(x = x, y = y, label = lab), 
    hjust = 0
  ) + 
  scale_x_continuous(breaks = seq(1960, 2025, 5)) + 
  labs(y = "Count", x = NULL) +
  ds4ling::ds4ling_bw_theme(base_size = 13)
```


@fig-journals (not included in the editorial) looks at the number of publications from the journal list we specified.

```{r}
#| label: fig-journals
#| fig-asp: 1
#| fig-cap: Distribution of entries from our sample based on journal.
dat |> 
  group_by(label) |> 
  count() |> 
  ungroup() |> 
  na.omit() |> 
  mutate(
    n_total = sum(n), 
    prop = n / n_total, 
    label = fct_reorder(label, .x = n)
  ) |> 
  ggplot() + 
  aes(y = label, x = prop) + 
  geom_segment(aes(x = 0, xend = prop), linewidth = 2) + 
  geom_point(size = 4, pch = 21, fill = "#cc0033", color = "white") + 
  geom_text(
    data = hits_df(x = 0.3, y = 1.75), 
    aes(x = x, y = y, label = lab), 
    hjust = 1
  ) + 
  labs(x = "Proportion", y = NULL) + 
  ds4ling::ds4ling_bw_theme(base_size = 13) + 
  theme(axis.text.y = element_text(size = 7))
```


We searched the title, abstracts, and kewords for (1) evidence of what language each article was studying, and (2) evidence that the article was about SLA/Bilingualism. 
For (1), we used the [worldlanguages](www.jvcasillas.com/worldlanguages) package to get a list of language names (scraped from https://en.wikipedia.org/wiki/List_of_language_names). 
The list includes 599 languages. 
For (2), we searched the title, abstract, and keywords for any of the following terms: 
- Second Language Acquisition
- SLA
- Second language
- L2
- Bilingualism
- Multilingualism
- Language acquisition
- Language learning

@fig-langs shows the porportion of the top 20 languages we could identify in our sample. 

```{r}
#| label: fig-langs
#| fig-asp: 0.6
#| fig-cap: Proportion of top 20 languages from scopus sample.

langs <- str_to_lower(language_names$language_en)
langs <- langs[!(langs %in% c("even", "e"))]
langs_pattern <- str_c(langs, collapse = "|")


l2_terms <- c(
  "sla", "second language", "l2", "bilingualism", 
  "language acquisition", "second language acquisition", 
  "multilingualism", "language learning"
)

l2_pattern <- str_c(l2_terms, collapse = "|")

lang_info <- dat |> 
  mutate(
    title = str_to_lower(title), 
    abstract = str_to_lower(abstract), 
    kw = str_to_lower(kw), 
    is_l2t = str_detect(title, l2_pattern), 
    is_l2a = str_detect(abstract, l2_pattern), 
    is_l2k = str_detect(kw, l2_pattern) 
  ) |> 
  mutate(
    is_l2a = if_else(is.na(is_l2a), FALSE, is_l2a), 
    is_l2 = if_else(is_l2t == TRUE | is_l2a == TRUE, 1, 0)
  ) |>
  select(year, doi, title, kw, abstract, is_l2) |> 
  unite(
    col = mix, 
    title, kw, abstract, 
    sep = " 999 ", remove = F
  ) |> 
  unnest_tokens(output = words, input = mix) |> 
  mutate(
    words = str_to_lower(words), 
    words = if_else(words == "chinese", "mandarin", words)
  ) |> 
  select(year, doi, title, words, is_l2)

title_words <- lang_info |> 
  filter(words %in% langs) |> 
  group_by(title, words) |> 
  count() |> 
  ungroup()

p_tw <- title_words |> 
  group_by(words) |> 
  count() |> 
  arrange(desc(n)) |> 
  ungroup() |> 
  mutate(n_total = sum(n), prop = n / n_total) |> 
  slice(1:20) |> 
  mutate(
    words = str_to_title(words), 
    words = fct_reorder(words, prop, max)
  ) |> 
  ggplot() + 
  aes(x = prop, y = words) + 
  geom_segment(aes(x = 0, xend = prop), linewidth = 1.75) + 
  geom_point(size = 4.25, pch = 21, fill = "#cc0033", color = "white") + 
  geom_text(aes(label = n), color = "white", size = 1.5) + 
  labs(x = "Proportion", y = NULL) + 
  ds4ling::ds4ling_bw_theme(base_size = 13) + 
  theme(axis.text.y = element_text(size = 9)) + 
  NULL

p_tw
```

@fig-donut (Figure 2 in the editorial) illustrates how the majority of articles come from English, Mandarin, German or Spanish. 

```{r}
#| label: fig-donut
#| fig-asp: 1
#| fig-cap: Proportion of Scopus articles on information structure by language, 1960-2024. The inner rings represent the disproportionate distribution between English, Mandarin, German, and Spanish versus all other languages in the sample (n = 110). The outer ring illustrates the proportion of studies related to SLA/Bilingualism (opaque sections, approx. 25%) versus studies involving monolinguals (non-opaque sections, 75%). 

totals <- lang_info |> 
  filter(words %in% langs) |> 
  group_by(title, words) |> 
  count() |> 
  group_by(words) |> 
  count() |> 
  arrange(desc(n)) |> 
  ungroup() |> 
  mutate(
    n_total = sum(n), 
    prop = n / n_total, 
    is_primary = if_else(
      words %in% c("english", "mandarin", "german", "spanish"), 
      "yes", "no"
    )
  ) |> 
  arrange(desc(prop))

lang_l2_counts <- lang_info |> 
  filter(words %in% langs) |> 
  group_by(title, words, is_l2) |> 
  count() |> 
  group_by(words, is_l2) |> 
  count() |> 
  ungroup() |> 
  arrange(is_l2, desc(n)) |> 
  complete(words, is_l2, fill = list(n = 0)) |> 
  rename(langs = words, num = n) |> 
  mutate(is_l2 = if_else(is_l2 == 0, "no", "yes"))

order <- lang_l2_counts |> 
  group_by(langs) |> 
  summarise(n = sum(num), .groups = "drop") |> 
  arrange(desc(n)) |>
  mutate(xmin = c(0, cumsum(head(n, -1))),
         xmax = cumsum(n), ymin = 5, ymax = 5) |> 
  transmute(langs, order = row_number())

inner <- title_words |> 
  select(langs = words, n) |> 
  group_by(langs) |> 
  count() |> 
  ungroup() |> 
  mutate(
    num = n, 
    is_primary = if_else(
      langs %in% c("english", "mandarin", "german", "spanish"), 
      "yes", "no"
    ) 
  ) |> 
  group_by(is_primary) |> 
  summarise(n = sum(num), .groups = "drop") |> 
  arrange(desc(n)) |> 
  mutate(
    prop = n / sum(n), 
    xmin = c(0, cumsum(head(prop, -1))),
    xmax = cumsum(prop), 
    ymin = 3, ymax = 4
  ) |> 
  reframe(
    is_primary = is_primary, langs = "all", 
    is_l2 = "combined", n = n,
    xmin = xmin, xmax = xmax,
    ymin = ymin, ymax = ymax, 
    perc = scales::percent(n / sum(n))
  ) 

middle <- totals |> 
  select(langs = words, num = n) |> 
  group_by(langs) |> 
  summarise(n = sum(num), .groups = "drop") |> 
  mutate(is_primary = if_else(
    langs %in% c("english", "mandarin", "german", "spanish"), 
    true = "yes", false = "no")
    ) |>
  arrange(desc(n)) |>
  mutate(
    prop = n / sum(n), 
    xmin = c(0, cumsum(head(prop, -1))),
    xmax = cumsum(prop), ymin = 4, ymax = 5) |> 
  mutate(
    is_primary = if_else(
      langs %in% c("english", "mandarin", "german", "spanish"), 
      "yes", "no"
    ) 
  ) |> 
  reframe(
    is_primary = is_primary, langs = langs, 
    is_l2 = "combined", n = n, 
    xmin = xmin, xmax = xmax,
    ymin = ymin, ymax = ymax, 
    perc = scales::percent(n / sum(n))
  ) 

outer <- lang_l2_counts |> 
  mutate(
    is_primary = if_else(
      langs %in% c("english", "mandarin", "german", "spanish"), 
      "yes", "no"
    ) 
  ) |> 
  left_join(order, by = "langs") |> 
  arrange(order) |> 
  mutate(
    prop = num / sum(num), 
    xmin = c(0, cumsum(head(prop, -1))),
    xmax = cumsum(prop), ymin = 5, ymax = 6
  ) |> 
  reframe(
    is_primary = is_primary, langs = langs, 
    is_l2 = is_l2, n = num, 
    xmin = xmin, xmax = xmax,
    ymin = ymin, ymax = ymax, 
    perc = scales::percent(n / sum(n))
  )

p_donut <- bind_rows(inner, middle, outer) |>
  mutate(
    is_primary = fct_relevel(is_primary, "yes"), 
    label = str_to_title(langs), 
    label = if_else(langs == "all", perc, label), 
    label_alpha = if_else(n > 50, 1, NA), 
    label_n = case_when(
      perc  == "52%" ~ 0.25,
      perc  == "48%" ~ 0.75, 
      label == "English" ~ 0.18, 
      label == "Mandarin" ~ 0.35, 
      label == "German" ~ 0.43, 
      label == "Spanish" ~ 0.49
    ), 
    inner_outer = if_else(label %in% c("52%", "48%"), 3.5, 4.5), 
    angle = case_when(
      label == "English" ~ 0, 
      label == "Mandarin" ~ -35, 
      label == "German" ~ -60, 
      label == "Spanish" ~ -90, 
      TRUE ~ 0
    )
  ) |> 
  ggplot() +
  geom_rect(
    aes(
      xmin = xmin, xmax = xmax, alpha = is_l2, 
      ymin = ymin, ymax = ymax, fill = is_primary
    ),
    color = "grey95", 
  ) + 
  geom_text(
    aes(x = label_n, y = inner_outer, label = label, angle = angle), 
    size = 3.5, color = "white"
  ) + 
  scale_y_continuous(limits = c(2.5, 6)) +
  scale_alpha_manual(
    name = NULL, values = c(1, 1, 0.5), 
    guide = "none") + 
  scale_fill_manual(
    name = NULL, values = c("#cc0033", "#888888"), 
    labels = c("English, Mandarin\nGerman, Spanish", 
               "Other languages\n(n=106)")
  ) + 
  coord_polar() + 
  theme_void(base_family = "Palatino") + 
  theme(
    legend.position = "bottom", 
    legend.key.spacing.y = unit(0.5, 'lines'), 
    legend.text = element_text(hjust = 0.5)
  )

p_donut
```


Approximately 25% of our sample could be identified as dealing with SLA/Bilingualism (See @fig-l2-prevalence and Table 1). 

```{r}
#| label: fig-l2-prevalence
#| fig-asp: 1.0
#| fig-height: 1
#| fig-width: 1

ns_transparent <- function(...) {
  list(
    theme_void(...), 
    theme(strip.text.x = element_text(size = rel(1.5)))
  )
}

plot_donut <- function(
  data, ymax = ymax, ymin = ymin, xmax = 4, 
  xmin = 3, fill = category, title = title, 
  subtitle = subtitle, ...) {
  ggplot(data) + 
  aes(
    ymax = ymax, ymin = ymin, 
    xmax = 4, xmin = 3, fill = category
  ) +
  geom_rect(show.legend = F, aes(alpha = alpha)) +
  geom_text(
    aes(x = label_pos, y = label_pos, label = label, 
        size = label_size), 
    color = "black", show.legend = F) + 
  scale_size_identity() + 
  scale_fill_manual(values = c("#cc0033", "black")) +
  scale_color_manual(values = c("#cc0033", "black")) +
  scale_alpha_identity() + 
  coord_polar(theta = "y") +
  xlim(c(-1, 4)) + 
  labs(title = title, subtitle = subtitle) + 
  ns_transparent(...) + 
  theme(plot.title = element_text(hjust=0.5), 
    plot.subtitle = element_text(hjust = 0.5))
}

l2_props <- outer |> 
  group_by(is_l2) |> 
  mutate(prop = n / sum(n)) |> 
  summarize(n = sum(n)) |> 
  mutate(n_total = sum(n), prop = n / n_total) |> 
  arrange(n)

l2_donut <- tibble(
  category = c("A", "B"),
  count = l2_props$prop * 100, 
  fraction = count / 100, 
  ymax = cumsum(fraction), 
  ymin = c(0, head(ymax, n = -1)), 
  label_pos = c(0, 0.5), 
  label_size = c(5, 2), 
  label = c("25.4%", "related to L2"), 
  alpha = c(1, 0.15)
  ) |> 
  plot_donut(base_size = 20, title = NULL, subtitle = NULL)

l2_donut
```

```{r}
#| label: tbl-l2-prevalence

l2_props |> 
  knitr::kable(label = "tbl-l2-prevalence")
```

# References

- Carminati, M. N. (2002). The processing of Italian subject pronouns. Boston: University of Massachusetts.
- Ferreira, F., Engelhardt, P. E., and Jones, M. W. (2009). “Good enough language processing: a satisfying approach” in Proceedings of the 31st annual conference of the cognitive science society. eds. N. Taatgen, H. Rijn, J. Nerbonne and L. Schomaker (Austin, TX: Cognitive Science Society), 413–418.
- Féry, C., & Ishihara, S. (Eds.). (2016). The Oxford handbook of information structure. Oxford, Oxford University Press.
- Krifka, M. (2008). Basic notions of information structure. Acta Linguistica Hungarica, 55(3–4), 243–276. 
- Krifka, M., & Musan, R. (Eds.). (2012). The expression of information structure. (Berlin: De Gruyter Mouton).
- Sorace, A. (2011). Pinning down the concept of “interface” in bilingualism. Linguis. Approach. Bilingual. 1, 1–33. 
